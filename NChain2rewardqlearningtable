def q_learning_with_table(env, num_episodes=500):
    q_table = np.zeros((5, 2))
    y = 0.95
    lr = 0.8
    for i in range(num_episodes):
        s = env.reset()
        done = False
        while not done:
            if np.sum(q_table[s,:]) == 0:
                # make a random selection of actions
                a = np.random.randint(0, 2)
            else:
                # select the action with largest q value in state s
                a = np.argmax(q_table[s, :])
            new_s, r, done, _ = env.step(a)
            q_table[s, a] += r + lr*(y*np.max(q_table[new_s, :]) - q_table[s, a])
            s = new_s
    return q_table
    #This output is strange, isn’t it? Again, we would expect at least the state 4 –
    #action 0 combination to have the highest Q score, but it doesn’t.
    #We might also expect the reward from this action in this state to have cascaded down through the states 0 to 3.
    # Something has clearly gone wrong – and the answer is that there isn’t enough exploration going on within the agent training method.
